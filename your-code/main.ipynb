{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Guys, I recognise the code on the last exercises may be cleaner, but I think I improved as I went\n",
    "### and I did not revise everything before...\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "# !pip install lxml\n",
    "# !pip install html5lib\n",
    "import re\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools or clicking in 'Inspect' on any browser. Here is an example:\n",
    "\n",
    "![title](example_1.png)\n",
    "\n",
    "2. Use BeautifulSoup `find_all()` to extract all the html elements that contain the developer names. Hint: pass in the `attrs` parameter to specify the class.\n",
    "\n",
    "3. Loop through the elements found and get the text for each of them.\n",
    "\n",
    "4. While you are at it, use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names. Hint: you may also use `.get_text()` instead of `.text` and pass in the desired parameters to do some string manipulation (check the documentation).\n",
    "\n",
    "5. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "text = soup.find_all('div',{'class':'col-md-6'})\n",
    "\n",
    "\n",
    "# There are some funny things under divs of this class which don't have \n",
    "# Trending Developer within them, let's take them out first.\n",
    "rel_text = []\n",
    "found = False\n",
    "for my_list_element in text:\n",
    "    for my_string in my_list_element:\n",
    "         if str(my_string).find('\"TRENDING_DEVELOPER\"') != -1:\n",
    "            rel_text.append(my_list_element)\n",
    "            break\n",
    "# Parsing...\n",
    "name_username = []\n",
    "for i in range(len(rel_text)):\n",
    "    my_username_parse = [element.strip() for element in rel_text[i].get_text().split(\"\\n\") if element != '']\n",
    "    if len(my_username_parse) > 2:\n",
    "        name_username.append(my_username_parse[0] + \" (\" + my_username_parse[2] + \")\")\n",
    "    else:\n",
    "        name_username.append(my_username_parse[0])\n",
    "        \n",
    "# name_username"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# Exact same logic as above, with a more granular class...\n",
    "text = soup.find_all('h1',{'class':\"h3 lh-condensed\"})\n",
    "text[0].get_text()\n",
    "\n",
    "my_repos = []\n",
    "for i in range(len(text)):\n",
    "    my_repos_parse = [element.strip() for element in text[i].get_text().replace(\"/\",\"\").split(\"\\n\") if element != '']\n",
    "    my_repos.append(my_repos_parse[0] + \" (\" + my_repos_parse[1] + \")\")\n",
    "\n",
    "# my_repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Display all the image links from Walt Disney wikipedia page.\n",
    "Hint: use `.get()` to access information inside tags. Check out the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "\n",
    "images = []\n",
    "for img in soup.findAll('img'):\n",
    "    images.append(img.get('src'))\n",
    "\n",
    "# images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ar': 'ar.wikipedia.org',\n",
       " 'de': 'de.wikipedia.org',\n",
       " 'en': 'en.wikipedia.org',\n",
       " 'es': 'es.wikipedia.org',\n",
       " 'fr': 'fr.wikipedia.org',\n",
       " 'it': 'it.wikipedia.org',\n",
       " 'arz': 'arz.wikipedia.org',\n",
       " 'nl': 'nl.wikipedia.org',\n",
       " 'ja': 'ja.wikipedia.org',\n",
       " 'pl': 'pl.wikipedia.org',\n",
       " 'pt': 'pt.wikipedia.org',\n",
       " 'ru': 'ru.wikipedia.org',\n",
       " 'ceb': 'ceb.wikipedia.org',\n",
       " 'sv': 'sv.wikipedia.org',\n",
       " 'uk': 'uk.wikipedia.org',\n",
       " 'vi': 'vi.wikipedia.org',\n",
       " 'war': 'war.wikipedia.org',\n",
       " 'zh': 'zh.wikipedia.org',\n",
       " 'ast': 'ast.wikipedia.org',\n",
       " 'az': 'az.wikipedia.org',\n",
       " 'bg': 'bg.wikipedia.org',\n",
       " 'zh-min-nan': 'zh-min-nan.wikipedia.org',\n",
       " 'be': 'be.wikipedia.org',\n",
       " 'ca': 'ca.wikipedia.org',\n",
       " 'cs': 'cs.wikipedia.org',\n",
       " 'cy': 'cy.wikipedia.org',\n",
       " 'da': 'da.wikipedia.org',\n",
       " 'et': 'et.wikipedia.org',\n",
       " 'el': 'el.wikipedia.org',\n",
       " 'eo': 'eo.wikipedia.org',\n",
       " 'eu': 'eu.wikipedia.org',\n",
       " 'fa': 'fa.wikipedia.org',\n",
       " 'gl': 'gl.wikipedia.org',\n",
       " 'hy': 'hy.wikipedia.org',\n",
       " 'hi': 'hi.wikipedia.org',\n",
       " 'hr': 'hr.wikipedia.org',\n",
       " 'id': 'id.wikipedia.org',\n",
       " 'he': 'he.wikipedia.org',\n",
       " 'ka': 'ka.wikipedia.org',\n",
       " 'la': 'la.wikipedia.org',\n",
       " 'lv': 'lv.wikipedia.org',\n",
       " 'lt': 'lt.wikipedia.org',\n",
       " 'hu': 'hu.wikipedia.org',\n",
       " 'mk': 'mk.wikipedia.org',\n",
       " 'ms': 'ms.wikipedia.org',\n",
       " 'min': 'min.wikipedia.org',\n",
       " 'no': 'no.wikipedia.org',\n",
       " 'nn': 'nn.wikipedia.org',\n",
       " 'ce': 'ce.wikipedia.org',\n",
       " 'uz': 'uz.wikipedia.org',\n",
       " 'kk': 'kk.wikipedia.org',\n",
       " 'ro': 'ro.wikipedia.org',\n",
       " 'simple': 'simple.wikipedia.org',\n",
       " 'sk': 'sk.wikipedia.org',\n",
       " 'sl': 'sl.wikipedia.org',\n",
       " 'sr': 'sr.wikipedia.org',\n",
       " 'sh': 'sh.wikipedia.org',\n",
       " 'fi': 'fi.wikipedia.org',\n",
       " 'ta': 'ta.wikipedia.org',\n",
       " 'tt': 'tt.wikipedia.org',\n",
       " 'th': 'th.wikipedia.org',\n",
       " 'tg': 'tg.wikipedia.org',\n",
       " 'azb': 'azb.wikipedia.org',\n",
       " 'tr': 'tr.wikipedia.org',\n",
       " 'ur': 'ur.wikipedia.org',\n",
       " 'vo': 'vo.wikipedia.org',\n",
       " 'ko': 'ko.wikipedia.org',\n",
       " 'ace': 'ace.wikipedia.org',\n",
       " 'af': 'af.wikipedia.org',\n",
       " 'als': 'als.wikipedia.org',\n",
       " 'am': 'am.wikipedia.org',\n",
       " 'an': 'an.wikipedia.org',\n",
       " 'bn': 'bn.wikipedia.org',\n",
       " 'map-bms': 'map-bms.wikipedia.org',\n",
       " 'ba': 'ba.wikipedia.org',\n",
       " 'be-tarask': 'be-tarask.wikipedia.org',\n",
       " 'bcl': 'bcl.wikipedia.org',\n",
       " 'bpy': 'bpy.wikipedia.org',\n",
       " 'bar': 'bar.wikipedia.org',\n",
       " 'bs': 'bs.wikipedia.org',\n",
       " 'br': 'br.wikipedia.org',\n",
       " 'cv': 'cv.wikipedia.org',\n",
       " 'nv': 'nv.wikipedia.org',\n",
       " 'eml': 'eml.wikipedia.org',\n",
       " 'fo': 'fo.wikipedia.org',\n",
       " 'fy': 'fy.wikipedia.org',\n",
       " 'ga': 'ga.wikipedia.org',\n",
       " 'gd': 'gd.wikipedia.org',\n",
       " 'gu': 'gu.wikipedia.org',\n",
       " 'hsb': 'hsb.wikipedia.org',\n",
       " 'io': 'io.wikipedia.org',\n",
       " 'ilo': 'ilo.wikipedia.org',\n",
       " 'ia': 'ia.wikipedia.org',\n",
       " 'os': 'os.wikipedia.org',\n",
       " 'is': 'is.wikipedia.org',\n",
       " 'jv': 'jv.wikipedia.org',\n",
       " 'kn': 'kn.wikipedia.org',\n",
       " 'ht': 'ht.wikipedia.org',\n",
       " 'ku': 'ku.wikipedia.org',\n",
       " 'ckb': 'ckb.wikipedia.org',\n",
       " 'ky': 'ky.wikipedia.org',\n",
       " 'mrj': 'mrj.wikipedia.org',\n",
       " 'lb': 'lb.wikipedia.org',\n",
       " 'li': 'li.wikipedia.org',\n",
       " 'lmo': 'lmo.wikipedia.org',\n",
       " 'mai': 'mai.wikipedia.org',\n",
       " 'mg': 'mg.wikipedia.org',\n",
       " 'ml': 'ml.wikipedia.org',\n",
       " 'zh-classical': 'zh-classical.wikipedia.org',\n",
       " 'mr': 'mr.wikipedia.org',\n",
       " 'xmf': 'xmf.wikipedia.org',\n",
       " 'mzn': 'mzn.wikipedia.org',\n",
       " 'cdo': 'cdo.wikipedia.org',\n",
       " 'mn': 'mn.wikipedia.org',\n",
       " 'my': 'my.wikipedia.org',\n",
       " 'new': 'new.wikipedia.org',\n",
       " 'ne': 'ne.wikipedia.org',\n",
       " 'nap': 'nap.wikipedia.org',\n",
       " 'frr': 'frr.wikipedia.org',\n",
       " 'oc': 'oc.wikipedia.org',\n",
       " 'mhr': 'mhr.wikipedia.org',\n",
       " 'or': 'or.wikipedia.org',\n",
       " 'pa': 'pa.wikipedia.org',\n",
       " 'pnb': 'pnb.wikipedia.org',\n",
       " 'ps': 'ps.wikipedia.org',\n",
       " 'pms': 'pms.wikipedia.org',\n",
       " 'nds': 'nds.wikipedia.org',\n",
       " 'qu': 'qu.wikipedia.org',\n",
       " 'sa': 'sa.wikipedia.org',\n",
       " 'sah': 'sah.wikipedia.org',\n",
       " 'sco': 'sco.wikipedia.org',\n",
       " 'sq': 'sq.wikipedia.org',\n",
       " 'scn': 'scn.wikipedia.org',\n",
       " 'si': 'si.wikipedia.org',\n",
       " 'sd': 'sd.wikipedia.org',\n",
       " 'szl': 'szl.wikipedia.org',\n",
       " 'su': 'su.wikipedia.org',\n",
       " 'sw': 'sw.wikipedia.org',\n",
       " 'tl': 'tl.wikipedia.org',\n",
       " 'te': 'te.wikipedia.org',\n",
       " 'bug': 'bug.wikipedia.org',\n",
       " 'vec': 'vec.wikipedia.org',\n",
       " 'wa': 'wa.wikipedia.org',\n",
       " 'yi': 'yi.wikipedia.org',\n",
       " 'yo': 'yo.wikipedia.org',\n",
       " 'diq': 'diq.wikipedia.org',\n",
       " 'bat-smg': 'bat-smg.wikipedia.org',\n",
       " 'kbd': 'kbd.wikipedia.org',\n",
       " 'ang': 'ang.wikipedia.org',\n",
       " 'ak': 'ak.wikipedia.org',\n",
       " 'ab': 'ab.wikipedia.org',\n",
       " 'hyw': 'hyw.wikipedia.org',\n",
       " 'roa-rup': 'roa-rup.wikipedia.org',\n",
       " 'frp': 'frp.wikipedia.org',\n",
       " 'ig': 'ig.wikipedia.org',\n",
       " 'arc': 'arc.wikipedia.org',\n",
       " 'gn': 'gn.wikipedia.org',\n",
       " 'av': 'av.wikipedia.org',\n",
       " 'ay': 'ay.wikipedia.org',\n",
       " 'bjn': 'bjn.wikipedia.org',\n",
       " 'bh': 'bh.wikipedia.org',\n",
       " 'bi': 'bi.wikipedia.org',\n",
       " 'bo': 'bo.wikipedia.org',\n",
       " 'bxr': 'bxr.wikipedia.org',\n",
       " 'cbk-zam': 'cbk-zam.wikipedia.org',\n",
       " 'co': 'co.wikipedia.org',\n",
       " 'za': 'za.wikipedia.org',\n",
       " 'se': 'se.wikipedia.org',\n",
       " 'pdc': 'pdc.wikipedia.org',\n",
       " 'dsb': 'dsb.wikipedia.org',\n",
       " 'myv': 'myv.wikipedia.org',\n",
       " 'ext': 'ext.wikipedia.org',\n",
       " 'hif': 'hif.wikipedia.org',\n",
       " 'fur': 'fur.wikipedia.org',\n",
       " 'gv': 'gv.wikipedia.org',\n",
       " 'gag': 'gag.wikipedia.org',\n",
       " 'ki': 'ki.wikipedia.org',\n",
       " 'glk': 'glk.wikipedia.org',\n",
       " 'hak': 'hak.wikipedia.org',\n",
       " 'xal': 'xal.wikipedia.org',\n",
       " 'ha': 'ha.wikipedia.org',\n",
       " 'haw': 'haw.wikipedia.org',\n",
       " 'ie': 'ie.wikipedia.org',\n",
       " 'pam': 'pam.wikipedia.org',\n",
       " 'csb': 'csb.wikipedia.org',\n",
       " 'kw': 'kw.wikipedia.org',\n",
       " 'km': 'km.wikipedia.org',\n",
       " 'rw': 'rw.wikipedia.org',\n",
       " 'kv': 'kv.wikipedia.org',\n",
       " 'kg': 'kg.wikipedia.org',\n",
       " 'gom': 'gom.wikipedia.org',\n",
       " 'gcr': 'gcr.wikipedia.org',\n",
       " 'lo': 'lo.wikipedia.org',\n",
       " 'lad': 'lad.wikipedia.org',\n",
       " 'lbe': 'lbe.wikipedia.org',\n",
       " 'ltg': 'ltg.wikipedia.org',\n",
       " 'lez': 'lez.wikipedia.org',\n",
       " 'ln': 'ln.wikipedia.org',\n",
       " 'jbo': 'jbo.wikipedia.org',\n",
       " 'lrc': 'lrc.wikipedia.org',\n",
       " 'lg': 'lg.wikipedia.org',\n",
       " 'lij': 'lij.wikipedia.org',\n",
       " 'mt': 'mt.wikipedia.org',\n",
       " 'ty': 'ty.wikipedia.org',\n",
       " 'mi': 'mi.wikipedia.org',\n",
       " 'mwl': 'mwl.wikipedia.org',\n",
       " 'mdf': 'mdf.wikipedia.org',\n",
       " 'fj': 'fj.wikipedia.org',\n",
       " 'nah': 'nah.wikipedia.org',\n",
       " 'na': 'na.wikipedia.org',\n",
       " 'nds-nl': 'nds-nl.wikipedia.org',\n",
       " 'nrm': 'nrm.wikipedia.org',\n",
       " 'nov': 'nov.wikipedia.org',\n",
       " 'as': 'as.wikipedia.org',\n",
       " 'pi': 'pi.wikipedia.org',\n",
       " 'pag': 'pag.wikipedia.org',\n",
       " 'pap': 'pap.wikipedia.org',\n",
       " 'koi': 'koi.wikipedia.org',\n",
       " 'pfl': 'pfl.wikipedia.org',\n",
       " 'pcd': 'pcd.wikipedia.org',\n",
       " 'krc': 'krc.wikipedia.org',\n",
       " 'kaa': 'kaa.wikipedia.org',\n",
       " 'crh': 'crh.wikipedia.org',\n",
       " 'ksh': 'ksh.wikipedia.org',\n",
       " 'rm': 'rm.wikipedia.org',\n",
       " 'rue': 'rue.wikipedia.org',\n",
       " 'sc': 'sc.wikipedia.org',\n",
       " 'stq': 'stq.wikipedia.org',\n",
       " 'nso': 'nso.wikipedia.org',\n",
       " 'sn': 'sn.wikipedia.org',\n",
       " 'so': 'so.wikipedia.org',\n",
       " 'srn': 'srn.wikipedia.org',\n",
       " 'kab': 'kab.wikipedia.org',\n",
       " 'roa-tara': 'roa-tara.wikipedia.org',\n",
       " 'tet': 'tet.wikipedia.org',\n",
       " 'tpi': 'tpi.wikipedia.org',\n",
       " 'to': 'to.wikipedia.org',\n",
       " 'tk': 'tk.wikipedia.org',\n",
       " 'tyv': 'tyv.wikipedia.org',\n",
       " 'udm': 'udm.wikipedia.org',\n",
       " 'ug': 'ug.wikipedia.org',\n",
       " 'vep': 'vep.wikipedia.org',\n",
       " 'fiu-vro': 'fiu-vro.wikipedia.org',\n",
       " 'vls': 'vls.wikipedia.org',\n",
       " 'wo': 'wo.wikipedia.org',\n",
       " 'xh': 'xh.wikipedia.org',\n",
       " 'zea': 'zea.wikipedia.org',\n",
       " 'zu': 'zu.wikipedia.org',\n",
       " 'dv': 'dv.wikipedia.org',\n",
       " 'bm': 'bm.wikipedia.org',\n",
       " 'ch': 'ch.wikipedia.org',\n",
       " 'ny': 'ny.wikipedia.org',\n",
       " 'ee': 'ee.wikipedia.org',\n",
       " 'ff': 'ff.wikipedia.org',\n",
       " 'got': 'got.wikipedia.org',\n",
       " 'iu': 'iu.wikipedia.org',\n",
       " 'ik': 'ik.wikipedia.org',\n",
       " 'kl': 'kl.wikipedia.org',\n",
       " 'ks': 'ks.wikipedia.org',\n",
       " 'cr': 'cr.wikipedia.org',\n",
       " 'pih': 'pih.wikipedia.org',\n",
       " 'om': 'om.wikipedia.org',\n",
       " 'pnt': 'pnt.wikipedia.org',\n",
       " 'dz': 'dz.wikipedia.org',\n",
       " 'rmy': 'rmy.wikipedia.org',\n",
       " 'rn': 'rn.wikipedia.org',\n",
       " 'sm': 'sm.wikipedia.org',\n",
       " 'sg': 'sg.wikipedia.org',\n",
       " 'st': 'st.wikipedia.org',\n",
       " 'tn': 'tn.wikipedia.org',\n",
       " 'cu': 'cu.wikipedia.org',\n",
       " 'ss': 'ss.wikipedia.org',\n",
       " 'chr': 'chr.wikipedia.org',\n",
       " 'chy': 'chy.wikipedia.org',\n",
       " 've': 've.wikipedia.org',\n",
       " 'ts': 'ts.wikipedia.org',\n",
       " 'tum': 'tum.wikipedia.org',\n",
       " 'tw': 'tw.wikipedia.org',\n",
       " 'ti': 'ti.wikipedia.org',\n",
       " 'nqo': 'nqo.wikipedia.org',\n",
       " 'meta': 'meta.wikimedia.org'}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "# HERE I WAS, GOING TO DO IT FOR ALL WIKIS EVEN THOSE WHICH ARE NOT IN THE MAIN PAGE...\n",
    "# AFTER MAKING A DICTIONARY OF ALL WIKIS I UNDERSTOOD THAT NOT ALL WIKIS FOLLOW THE SAME TAG STRUCTURE\n",
    "# AND JUST DID THOSE WHICH HAVE THE NUMBER ON THE FIRST PAGE OF WIKIPEDIA.ORG\n",
    "# ILL STUDY ARAB AFTER IRONHACK TO COMPLETE THIS LAB\n",
    "\n",
    "# html = requests.get(url).content\n",
    "# soup = BeautifulSoup(html, \"lxml\")\n",
    "# text = soup.find_all('li')\n",
    "\n",
    "# links_languages = [str(text[i]).split(\" \")[1] for i in range(len(text))]\n",
    "# links_languages = [line.replace('href=\"//','') for line in links_languages]\n",
    "# links_languages = [line.replace('lang=\"','') for line in links_languages]\n",
    "# links_languages = [line.replace('\"','') for line in links_languages]\n",
    "# links_languages = [line.replace('\"','') for line in links_languages]\n",
    "# links_languages = [line for line in links_languages if '.org' in line]\n",
    "# links_languages = [line.split(\"/\")[0] for line in links_languages]\n",
    "\n",
    "# links_languages\n",
    "# languages_shortname = [line.split('.')[0] for line in links_languages]\n",
    "# languages_shortname\n",
    "\n",
    "# wikipedias = list(zip(languages_shortname,links_languages))\n",
    "# dict(wikipedias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English 6 199 000+ articles',\n",
       " 'Español 1 643 000+ artículos',\n",
       " '日本語 1 240 000+ 記事',\n",
       " 'Deutsch 2 505 000+ Artikel',\n",
       " 'Русский 1 680 000+ статей',\n",
       " 'Français 2 273 000+ articles',\n",
       " 'Italiano 1 655 000+ voci',\n",
       " '中文 1 160 000+ 條目',\n",
       " 'Português 1 048 000+ artigos',\n",
       " 'Polski 1 441 000+ haseł']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "text = soup.find_all('div',{'class': 'central-featured'})\n",
    "text[0].get_text().replace(u'\\xa0', u' ').replace(u'\\n', u' ').strip().split(\"      \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Display the top 10 languages by number of native speakers stored in a pandas dataframe.\n",
    "Hint: After finding the correct table you want to analyse, you can use a nested **for** loop to find the elements row by row (check out the 'td' and 'tr' tags). <br>An easier way to do it is using pd.read_html(), check out documentation [here](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.read_html.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "data = pd.read_html(url)[0]\n",
    "# data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe.\n",
    "Hint: If you hover over the title of the movie, you should see the director's name. Can you find where it's stored in the html?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frank Darabont (dir.), Tim Robbins, Morgan Freeman'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Code below is more efficient (after Gonçalo dropped in ;) )\n",
    "\n",
    "# url = 'https://www.imdb.com/chart/top'\n",
    "# response = requests.get(url).content\n",
    "# soup = BeautifulSoup(response, 'lxml')\n",
    "\n",
    "# directors = soup.find_all('td',attrs = {'class':'titleColumn'})[0]\n",
    "# directors.find('a').get('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Initial Release</th>\n",
       "      <th>Director Name</th>\n",
       "      <th>IMDb Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather: Part II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angry Men</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>La battaglia di Algeri</td>\n",
       "      <td>1966</td>\n",
       "      <td>Gillo Pontecorvo</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Mandariinid</td>\n",
       "      <td>2013</td>\n",
       "      <td>Zaza Urushadze</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>The Circus</td>\n",
       "      <td>1928</td>\n",
       "      <td>Charles Chaplin</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>The Terminator</td>\n",
       "      <td>1984</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Koe no katachi</td>\n",
       "      <td>2016</td>\n",
       "      <td>Naoko Yamada</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Title  Initial Release         Director Name  \\\n",
       "0    The Shawshank Redemption             1994        Frank Darabont   \n",
       "1               The Godfather             1972  Francis Ford Coppola   \n",
       "2      The Godfather: Part II             1974  Francis Ford Coppola   \n",
       "3             The Dark Knight             2008     Christopher Nolan   \n",
       "4                   Angry Men             1957          Sidney Lumet   \n",
       "..                        ...              ...                   ...   \n",
       "245    La battaglia di Algeri             1966      Gillo Pontecorvo   \n",
       "246               Mandariinid             2013        Zaza Urushadze   \n",
       "247                The Circus             1928       Charles Chaplin   \n",
       "248            The Terminator             1984         James Cameron   \n",
       "249            Koe no katachi             2016          Naoko Yamada   \n",
       "\n",
       "     IMDb Rating  \n",
       "0            9.2  \n",
       "1            9.1  \n",
       "2            9.0  \n",
       "3            9.0  \n",
       "4            8.9  \n",
       "..           ...  \n",
       "245          8.0  \n",
       "246          8.0  \n",
       "247          8.0  \n",
       "248          8.0  \n",
       "249          8.0  \n",
       "\n",
       "[250 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'\n",
    "response = requests.get(url).content\n",
    "soup = BeautifulSoup(response, 'lxml')\n",
    "\n",
    "### Making a DF Column for Directors\n",
    "col_title = soup.select('td.titleColumn')\n",
    "directors = [str(col_title[i]).split('title=')[1].split('(dir.)')[0].replace('\"','').strip() for i in range(len(col_title))]\n",
    "directors = pd.DataFrame(directors)\n",
    "\n",
    "### Getting the Table and Adding Directors\n",
    "data = pd.read_html(url)[0]\n",
    "data['Director Name'] = directors\n",
    "\n",
    "### Making Regex Helper Functions\n",
    "def get_year_imdb(my_string):\n",
    "    return int(re.findall('\\d\\d\\d\\d',my_string)[0])\n",
    "\n",
    "def get_title_imdb(my_string):\n",
    "    regex_string = re.findall('[^. (\\d)]+',my_string)\n",
    "    return \" \".join(regex_string)\n",
    "\n",
    "### Adding new columns using Regex on Rank and Tittle\n",
    "data['Initial Release'] = list(map(get_year_imdb,data['Rank & Title']))\n",
    "data['Title'] = list(map(get_title_imdb,data['Rank & Title']))\n",
    "\n",
    "### Dropping Old columns\n",
    "data = data.drop(['Your Rating','Unnamed: 4','Unnamed: 0','Rank & Title'],axis=1)\n",
    "\n",
    "### Reordering\n",
    "data = data[['Title', 'Initial Release', 'Director Name', 'IMDb Rating']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Nightmare on Elm Street</td>\n",
       "      <td>1984</td>\n",
       "      <td>The monstrous spirit of a slain child murderer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Awakenings</td>\n",
       "      <td>1990</td>\n",
       "      <td>The victims of an encephalitis epidemic many y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A League of Their Own</td>\n",
       "      <td>1992</td>\n",
       "      <td>Two sisters join the first female professional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Bronx Tale</td>\n",
       "      <td>1993</td>\n",
       "      <td>A father becomes worried when a local gangster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Angels in the Outfield</td>\n",
       "      <td>1994</td>\n",
       "      <td>When a boy prays for a chance to have a family...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A Time to Kill</td>\n",
       "      <td>1996</td>\n",
       "      <td>In Canton, Mississippi, a fearless young lawye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Amistad</td>\n",
       "      <td>1997</td>\n",
       "      <td>In 1839, the revolt of Mende captives aboard a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Anaconda</td>\n",
       "      <td>1997</td>\n",
       "      <td>A \"National Geographic\" film crew is taken hos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A Cool, Dry Place</td>\n",
       "      <td>1998</td>\n",
       "      <td>Russell, single father balances his work as a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>American History X</td>\n",
       "      <td>1998</td>\n",
       "      <td>A former neo-nazi skinhead tries to prevent hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name  Year  \\\n",
       "0  A Nightmare on Elm Street  1984   \n",
       "1                 Awakenings  1990   \n",
       "2      A League of Their Own  1992   \n",
       "3               A Bronx Tale  1993   \n",
       "4     Angels in the Outfield  1994   \n",
       "5             A Time to Kill  1996   \n",
       "6                    Amistad  1997   \n",
       "7                   Anaconda  1997   \n",
       "8          A Cool, Dry Place  1998   \n",
       "9         American History X  1998   \n",
       "\n",
       "                                         Description  \n",
       "0  The monstrous spirit of a slain child murderer...  \n",
       "1  The victims of an encephalitis epidemic many y...  \n",
       "2  Two sisters join the first female professional...  \n",
       "3  A father becomes worried when a local gangster...  \n",
       "4  When a boy prays for a chance to have a family...  \n",
       "5  In Canton, Mississippi, a fearless young lawye...  \n",
       "6  In 1839, the revolt of Mende captives aboard a...  \n",
       "7  A \"National Geographic\" film crew is taken hos...  \n",
       "8  Russell, single father balances his work as a ...  \n",
       "9  A former neo-nazi skinhead tries to prevent hi...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'https://www.imdb.com/list/ls009796553/'\n",
    "response = requests.get(url).content\n",
    "soup = BeautifulSoup(response, 'lxml')\n",
    "\n",
    "### Defining the soups to be used\n",
    "my_class_lister_item_header = soup.find_all('h3',attrs = {'class':'lister-item-header'})[:10]\n",
    "my_unnamed_class = soup.find_all(\"p\", {\"class\":\"\"})[:10]\n",
    "\n",
    "### Getting movie year and movie name using previously defined regex functions\n",
    "movie_names_and_years = [my_class_lister_item_header[i].get_text().replace('\\n',\"\") for i in range(len(my_class_lister_item_header))]\n",
    "movie_year = list(map(get_year_imdb,movie_names_and_years))\n",
    "movie_name = list(map(get_title_imdb,movie_names_and_years))\n",
    "\n",
    "### Getting brief description from unammed class...\n",
    "movie_description = [my_unnamed_class[i].get_text() for i in range(len(my_unnamed_class))]\n",
    "movie_description = [movie_description[i].replace(\"\\n\",\"\").strip() for i in range(len(movie_description))]\n",
    "\n",
    "### Making the dataframe\n",
    "data = pd.DataFrame(list(zip(movie_name,movie_year,movie_description)),columns =['Name', 'Year', 'Description'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://openweathermap.org/current\n",
    "# city = input('Enter the city: ')\n",
    "# url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "## SKIPPED, see other Bonus Questions##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A Light in the Attic', '£51.77', 'In stock'),\n",
       " ('Tipping the Velvet', '£53.74', 'In stock'),\n",
       " ('Soumission', '£50.10', 'In stock'),\n",
       " ('Sharp Objects', '£47.82', 'In stock'),\n",
       " ('Sapiens: A Brief History of Humankind', '£54.23', 'In stock'),\n",
       " ('The Requiem Red', '£22.65', 'In stock'),\n",
       " ('The Dirty Little Secrets of Getting Your Dream Job', '£33.34', 'In stock'),\n",
       " ('The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull',\n",
       "  '£17.93',\n",
       "  'In stock'),\n",
       " ('The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics',\n",
       "  '£22.60',\n",
       "  'In stock'),\n",
       " ('The Black Maria', '£52.15', 'In stock'),\n",
       " ('Starving Hearts (Triangular Trade Trilogy, #1)', '£13.99', 'In stock'),\n",
       " (\"Shakespeare's Sonnets\", '£20.66', 'In stock'),\n",
       " ('Set Me Free', '£17.46', 'In stock'),\n",
       " (\"Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\",\n",
       "  '£52.29',\n",
       "  'In stock'),\n",
       " ('Rip it Up and Start Again', '£35.02', 'In stock'),\n",
       " ('Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991',\n",
       "  '£57.25',\n",
       "  'In stock'),\n",
       " ('Olio', '£23.88', 'In stock'),\n",
       " ('Mesaerion: The Best Science Fiction Stories 1800-1849',\n",
       "  '£37.59',\n",
       "  'In stock'),\n",
       " ('Libertarianism for Beginners', '£51.33', 'In stock'),\n",
       " (\"It's Only the Himalayas\", '£45.17', 'In stock')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "response = requests.get(url).content\n",
    "soup = BeautifulSoup(response, 'lxml')\n",
    "\n",
    "my_h3 = soup.find_all('h3',attrs = {'class':''})\n",
    "my_titles = [my_h3[i].find('a').get('title') for i in range(len(my_h3))]\n",
    "\n",
    "my_div = soup.find_all('div', attrs = {'class':\"product_price\"})\n",
    "my_prices = [my_div[i].find('p').get_text() for i in range(len(my_div))]\n",
    "\n",
    "my_stock = [my_div[i].get_text() for i in range(len(my_div))]\n",
    "my_stock = [my_stock[i].split('\\n')[5].strip() for i in range(len(my_stock))]\n",
    "\n",
    "my_answer = list(zip(my_titles,my_prices,my_stock))\n",
    "my_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 100 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe.\n",
    "***Hint:*** Here the displayed number of earthquakes per page is 20, but you can easily move to the next page by looping through the desired number of pages and adding it to the end of the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.emsc-csem.org/Earthquake/?view=1',\n",
       " 'https://www.emsc-csem.org/Earthquake/?view=2',\n",
       " 'https://www.emsc-csem.org/Earthquake/?view=3',\n",
       " 'https://www.emsc-csem.org/Earthquake/?view=4',\n",
       " 'https://www.emsc-csem.org/Earthquake/?view=5']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/?view='\n",
    "\n",
    "# This is how you will loop through each page:\n",
    "number_of_pages = int(100/20)\n",
    "each_page_urls = []\n",
    "\n",
    "for n in range(1, number_of_pages+1):\n",
    "    link = url+str(n)\n",
    "    each_page_urls.append(link)\n",
    "    \n",
    "each_page_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lat_Val</th>\n",
       "      <th>Lat_Let</th>\n",
       "      <th>Lon_Val</th>\n",
       "      <th>Lon_Let</th>\n",
       "      <th>Depth_Km</th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Region</th>\n",
       "      <th>Dates_str</th>\n",
       "      <th>Times_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.89</td>\n",
       "      <td>N</td>\n",
       "      <td>26.91</td>\n",
       "      <td>E</td>\n",
       "      <td>5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "      <td>[2020-11-21]</td>\n",
       "      <td>[17:44:30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61.70</td>\n",
       "      <td>N</td>\n",
       "      <td>148.09</td>\n",
       "      <td>W</td>\n",
       "      <td>18</td>\n",
       "      <td>3.7</td>\n",
       "      <td>SOUTHERN ALASKA</td>\n",
       "      <td>[2020-11-21]</td>\n",
       "      <td>[17:20:40]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.59</td>\n",
       "      <td>N</td>\n",
       "      <td>120.69</td>\n",
       "      <td>W</td>\n",
       "      <td>1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>NORTHERN CALIFORNIA</td>\n",
       "      <td>[2020-11-21]</td>\n",
       "      <td>[17:20:35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.10</td>\n",
       "      <td>N</td>\n",
       "      <td>120.37</td>\n",
       "      <td>E</td>\n",
       "      <td>105</td>\n",
       "      <td>4.3</td>\n",
       "      <td>LUZON, PHILIPPINES</td>\n",
       "      <td>[2020-11-21]</td>\n",
       "      <td>[17:06:17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.31</td>\n",
       "      <td>S</td>\n",
       "      <td>71.58</td>\n",
       "      <td>W</td>\n",
       "      <td>70</td>\n",
       "      <td>4.1</td>\n",
       "      <td>OFFSHORE VALPARAISO, CHILE</td>\n",
       "      <td>[2020-11-21]</td>\n",
       "      <td>[17:01:29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2.52</td>\n",
       "      <td>S</td>\n",
       "      <td>128.72</td>\n",
       "      <td>E</td>\n",
       "      <td>10</td>\n",
       "      <td>3.7</td>\n",
       "      <td>CERAM SEA, INDONESIA</td>\n",
       "      <td>[2020-11-20]</td>\n",
       "      <td>[16:29:10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>63.57</td>\n",
       "      <td>N</td>\n",
       "      <td>149.94</td>\n",
       "      <td>W</td>\n",
       "      <td>127</td>\n",
       "      <td>3.1</td>\n",
       "      <td>CENTRAL ALASKA</td>\n",
       "      <td>[2020-11-20]</td>\n",
       "      <td>[16:24:06]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2.76</td>\n",
       "      <td>S</td>\n",
       "      <td>128.82</td>\n",
       "      <td>E</td>\n",
       "      <td>10</td>\n",
       "      <td>4.3</td>\n",
       "      <td>CERAM SEA, INDONESIA</td>\n",
       "      <td>[2020-11-20]</td>\n",
       "      <td>[16:08:09]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2.71</td>\n",
       "      <td>S</td>\n",
       "      <td>128.94</td>\n",
       "      <td>E</td>\n",
       "      <td>10</td>\n",
       "      <td>4.5</td>\n",
       "      <td>CERAM SEA, INDONESIA</td>\n",
       "      <td>[2020-11-20]</td>\n",
       "      <td>[16:05:04]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>17.85</td>\n",
       "      <td>N</td>\n",
       "      <td>66.81</td>\n",
       "      <td>W</td>\n",
       "      <td>10</td>\n",
       "      <td>2.1</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "      <td>[2020-11-20]</td>\n",
       "      <td>[16:03:47]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Lat_Val Lat_Let Lon_Val Lon_Let Depth_Km Magnitude  \\\n",
       "0     37.89       N   26.91       E        5       2.5   \n",
       "1     61.70       N  148.09       W       18       3.7   \n",
       "2     39.59       N  120.69       W        1       2.2   \n",
       "3     14.10       N  120.37       E      105       4.3   \n",
       "4     32.31       S   71.58       W       70       4.1   \n",
       "..      ...     ...     ...     ...      ...       ...   \n",
       "245    2.52       S  128.72       E       10       3.7   \n",
       "246   63.57       N  149.94       W      127       3.1   \n",
       "247    2.76       S  128.82       E       10       4.3   \n",
       "248    2.71       S  128.94       E       10       4.5   \n",
       "249   17.85       N   66.81       W       10       2.1   \n",
       "\n",
       "                         Region     Dates_str   Times_str  \n",
       "0    DODECANESE ISLANDS, GREECE  [2020-11-21]  [17:44:30]  \n",
       "1               SOUTHERN ALASKA  [2020-11-21]  [17:20:40]  \n",
       "2           NORTHERN CALIFORNIA  [2020-11-21]  [17:20:35]  \n",
       "3            LUZON, PHILIPPINES  [2020-11-21]  [17:06:17]  \n",
       "4    OFFSHORE VALPARAISO, CHILE  [2020-11-21]  [17:01:29]  \n",
       "..                          ...           ...         ...  \n",
       "245        CERAM SEA, INDONESIA  [2020-11-20]  [16:29:10]  \n",
       "246              CENTRAL ALASKA  [2020-11-20]  [16:24:06]  \n",
       "247        CERAM SEA, INDONESIA  [2020-11-20]  [16:08:09]  \n",
       "248        CERAM SEA, INDONESIA  [2020-11-20]  [16:05:04]  \n",
       "249          PUERTO RICO REGION  [2020-11-20]  [16:03:47]  \n",
       "\n",
       "[250 rows x 9 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "# Since I saw 50 in each page I did it for 250 entries... just checking how to loop the pages...\n",
    "\n",
    "# Defining some regex functions\n",
    "def get_date_earthquakes(my_string):   \n",
    "    return re.findall('\\d\\d\\d\\d-\\d\\d-\\d\\d',my_string)\n",
    "def get_time_earthquakes(my_string):   \n",
    "    return re.findall('\\d\\d:\\d\\d:\\d\\d',my_string)\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "#Parsing everything to a DF...\n",
    "for i in range(len(each_page_urls)):\n",
    "    read = pd.read_html(each_page_urls[i])[3].head(50)\n",
    "    read.columns = read.columns.droplevel(0)\n",
    "    read = read[:].values\n",
    "    read = pd.DataFrame(read)\n",
    "    data = data.append(read, ignore_index = True) \n",
    "\n",
    "data.drop([0,1,2,9,12], axis=1, inplace = True)\n",
    "data.rename(columns={3: \"Time\", 4: \"Lat_Val\", 5: \"Lat_Let\", \n",
    "                     6: \"Lon_Val\", 7: \"Lon_Let\",8:\"Depth_Km\",\n",
    "                     10:\"Magnitude\",11:\"Region\"}, inplace = True)\n",
    "\n",
    "#Splitting times and dates...\n",
    "data[\"Time\"] = data[\"Time\"].astype(str)\n",
    "\n",
    "dates = list(map(get_date_earthquakes,list(data[\"Time\"])))\n",
    "data[\"Dates_str\"] = dates\n",
    "\n",
    "times = list(map(get_time_earthquakes,list(data[\"Time\"])))\n",
    "data[\"Times_str\"] = times\n",
    "\n",
    "data.drop(\"Time\",axis=1, inplace = True)\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
